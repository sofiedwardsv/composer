{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating t-SNE plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates t-SNE plots for visualizing the outputs of certain layers of the RoBERTa model, fragment 128, IMSLP pretrained on the fullpage data and unseen composers. It also prepares the unseen composer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from transformers import RobertaForSequenceClassification, RobertaConfig\n",
    "from train_utils import *\n",
    "import tokenizers\n",
    "from fastai.callbacks.hooks import *\n",
    "from plotnine import *\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare unseen composer data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we select our unseen composers, and select the corresponding bootleg scores from the entire IMSLP piano bootleg score dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imslp_bscores_filelist = 'imslp_bscores.list'\n",
    "data_path = Config.data_path()/'others'\n",
    "data_path.mkdir(exist_ok=True)\n",
    "composers = ['Tchaikovsky', 'Clementi', 'Pachelbel', 'Debussy', 'Joplin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bscore2textfile(infile, outfile, min_thresh = 100):\n",
    "    '''\n",
    "    Converts a bootleg score .pkl file to text and writes to the specified output file.\n",
    "    '''\n",
    "    with open(infile, 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    \n",
    "    with open(outfile, 'w') as fout:\n",
    "        for l in d: # each page\n",
    "            if len(l) > min_thresh: # to avoid filler pages\n",
    "                pageStr = ' '.join([str(i) for i in l])\n",
    "                fout.write(pageStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imslp2text(filelist, outdir):\n",
    "    from pathlib import Path\n",
    "    with open(filelist, 'r') as f:\n",
    "        for line in f:\n",
    "            bscorefile = Path(line.strip()) # path/composer/piece/283513.pkl\n",
    "            fileid = bscorefile.name[:-4] # e.g. 283513\n",
    "            composer = str(bscorefile.parent.parent.name).split(',')[0]\n",
    "            if composer in composers:\n",
    "                Path(outdir/f'{composer}').mkdir(exist_ok=True)\n",
    "                outfile = outdir/f'{composer}'/f'{fileid}.txt'\n",
    "                bscore2textfile(bscorefile, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "composer_data = defaultdict(int)\n",
    "with open(imslp_bscores_filelist) as f:\n",
    "    for line in f:\n",
    "        bscorefile = Path(line.strip()) # path/composer/piece/283513.pkl\n",
    "        composer = str(bscorefile.parent.parent.name).split(',')[0]\n",
    "        composer_data[composer] += 1\n",
    "\n",
    "data = {'composers': list(composer_data.keys()), 'counts' : list(composer_data.values())}\n",
    "composer_df = pd.DataFrame.from_dict(data)\n",
    "composer_df = composer_df.sort_values(by=['counts'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imslp2text(imslp_bscores_filelist, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we convert the data for the selected unseen composers into csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFullPageCSV(directory, outfile):\n",
    "    with open(outfile, 'w') as fout:\n",
    "        fout.write('label,text\\n')\n",
    "        for fileDir in directory.rglob('*.txt'): # others/composer/file_id.txt\n",
    "            composer = fileDir.parent.name\n",
    "            with open(fileDir, 'r') as text_file:\n",
    "                text = text_file.read()\n",
    "                fout.write(f'{composer},{text}\\n')\n",
    "    df = pd.read_csv(outfile)\n",
    "    df.dropna(inplace=True)\n",
    "    df.to_csv(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateFullPageCSV(data_path, data_path/'others.fullpage.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are using RoBERTa, we need to convert the data into the proper format for Transformer-based models. Instead of using decimal string representations, we represent each 62-bit bootleg score feature as a sequence of 8 one-byte characters. Rather than generating these from scratch, we will simply convert the existing files to the new format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertLineToCharSeq(line):\n",
    "    ints = [int(p) for p in line.split()]\n",
    "    result = ' '.join([int2charseq(i) for i in ints])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int2charseq(int64):\n",
    "    chars = ''\n",
    "    for i in range(8):\n",
    "        numshift = i * 8\n",
    "        charidx = (int64 >> numshift) & 255\n",
    "        chars += chr(19968 + charidx) # 19968 ensures that all chars are chinese characters (not newline, space, etc)\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertSingleCSVFile(infile, outfile):\n",
    "    '''\n",
    "    Convert .csv file with decimal string representation of bootleg score features to\n",
    "    a .csv file with byte character representation.\n",
    "    '''\n",
    "    with open(infile, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    with open(outfile, 'w') as fout:\n",
    "        for i, line in enumerate(lines):\n",
    "            if i==0: \n",
    "                fout.write(line) # header\n",
    "            else:\n",
    "                parts = line.strip().split(',')\n",
    "                feats = parts.pop()\n",
    "                charseq = convertLineToCharSeq(feats)\n",
    "                strToWrite = ','.join(parts) + ',' + charseq + '\\n'\n",
    "                fout.write(strToWrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertSingleCSVFile(data_path/'others.fullpage.csv', data_path/'others.fullpage128.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hook to store outputs of intermediate layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a PyTorch hook to store the outputs of a given layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoreHook(Callback):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.custom_hook = hook_output(module)\n",
    "        self.outputs = []\n",
    "        \n",
    "    def on_batch_end(self, train, **kwargs): \n",
    "        if (not train): self.outputs.append(self.custom_hook.stored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for making t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the outputs of our hook and then make a t-SNE plot of these outputs  the model's predicted composer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hook_outputs(hook, y, is_penult_layer=False):\n",
    "    \"\"\" given a hook for a model, get the hook outputs at the given layer \"\"\"\n",
    "    num_outputs = len(y)\n",
    "    outputs = []\n",
    "    for output in hook.outputs:\n",
    "        if is_penult_layer:\n",
    "            # get the first token for RoBERTa because that's what it uses for prediction\n",
    "            outputs.append(output[:, 0, :])\n",
    "        else:\n",
    "            outputs.append(output)\n",
    "    outputs = torch.cat(outputs)[-num_outputs:].cpu().numpy()\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tSNE(x, y):\n",
    "    num_classes = len(np.unique(y))\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    tsne_obj = tsne.fit_transform(x)\n",
    "    tsne_df = pd.DataFrame({'X': tsne_obj[:, 0],\n",
    "                           'Y': tsne_obj[:, 1],\n",
    "                           'composer': y})\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    legend_position = \"right\"\n",
    "    return (ggplot(tsne_df, aes(x=\"X\", y=\"Y\")) + \n",
    "        geom_point(aes(color = 'factor(composer)'), alpha=0.8) +\n",
    "        theme_bw() +\n",
    "        guides(colour = guide_legend(override_aes = {'alpha': 1})) +\n",
    "        theme(dpi=300, legend_title=element_blank(), legend_key=element_blank(), \n",
    "            axis_text_x = element_blank(),\n",
    "            axis_text_y = element_blank(),\n",
    "            axis_title_x = element_blank(),\n",
    "            axis_title_y = element_blank(),\n",
    "            axis_ticks = element_blank()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep databunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Config.data_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "seed = 42\n",
    "tok_model_dir = str(data_path/'bscore_lm/bpe_data/tokenizer_imslp')\n",
    "max_seq_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_tok = CustomTokenizer(TransformersBaseTokenizer, tok_model_dir, max_seq_len)\n",
    "transformer_base_tokenizer = TransformersBaseTokenizer(tok_model_dir, max_seq_len)\n",
    "transformer_vocab =  TransformersVocab(tokenizer = transformer_base_tokenizer._pretrained_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = transformer_base_tokenizer._pretrained_tokenizer.token_to_id('<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_path = data_path/'bscore_lm/bpe_data'\n",
    "train_df = pd.read_csv(bpe_path/'train128.char.csv')\n",
    "valid_df = pd.read_csv(bpe_path/'valid128.char.csv')\n",
    "test_df = pd.read_csv(bpe_path/'test128.char.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = TextDataBunch.from_df(bpe_path, train_df, valid_df, tokenizer=cust_tok, vocab=transformer_vocab,\n",
    "                                  include_bos=False, include_eos=False, pad_first=False, pad_idx=pad_idx, \n",
    "                                  bs=bs, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RoBERTa Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, config_class = RobertaForSequenceClassification, RobertaConfig\n",
    "model_path = str(bpe_path/'models/roberta_train-imslp_finetune-target_lm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_class.from_pretrained(model_path)\n",
    "config.num_labels = data_clas.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = model_class.from_pretrained(model_path, config = config)\n",
    "custom_transformer_model = RobertaModelWrapper(transformer_model, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# note: load_path will depend on which model you want to load\n",
    "load_path = str(bpe_path'models/roberta_train-imslp_finetune-target_clas_128')\n",
    "learner = Learner(data_clas, custom_transformer_model, metrics=[accuracy, FBeta(average='macro', beta=1)])\n",
    "learner.load(load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penult_layer_hook = StoreHook(list(learner.model.modules())[-8])\n",
    "last_layer_hook = StoreHook(list(learner.model.modules())[-1])\n",
    "learner.callbacks += [penult_layer_hook, last_layer_hook]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make databunches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fullpage_df = pd.read_csv(bpe_path/'train.fullpage.char.csv')\n",
    "valid_fullpage_df = pd.read_csv(bpe_path/'valid.fullpage.char.csv')\n",
    "test_fullpage_df = pd.read_csv(bpe_path/'test.fullpage.char.csv')\n",
    "\n",
    "data_clas_test = TextDataBunch.from_df(bpe_path, train_fullpage_df, valid_fullpage_df, test_fullpage_df,\n",
    "                                       tokenizer=cust_tok, vocab=transformer_vocab, include_bos=False, \n",
    "                                       include_eos=False, pad_first=False, pad_idx=pad_idx, bs=bs, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Config.data_path()/'others'\n",
    "data_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "others_df = pd.read_csv(data_path/'others.fullpage128.csv', index_col=[0])\n",
    "# put as much of the data as possible in train because we're using it all for evaluation\n",
    "np.random.seed(42)\n",
    "train, validate = np.split(others_df.sample(frac=1), [int(.99*len(others_df))])\n",
    "\n",
    "data_clas_test_others = TextDataBunch.from_df(data_path, train, validate,\n",
    "                                       tokenizer=cust_tok, vocab=transformer_vocab, include_bos=False, \n",
    "                                       include_eos=False, pad_first=False, pad_idx=pad_idx, bs=bs, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. T-SNE plot of full pages in test set, original 9 composers, penultimate layer (768 dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.data = data_clas_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, y = learner.get_preds(ds_type=DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_to_composer = {i: composer for i, composer in enumerate(learner.data.classes)}\n",
    "labels = [pred_to_composer[num.item()] for num in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = get_hook_outputs(penult_layer_hook, y, is_penult_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tSNE(points, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. T-SNE plot of full pages in test set, original 9 composers, last layer (9 dim, before softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = get_hook_outputs(last_layer_hook, y, is_penult_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tSNE(points, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  T-SNE plot of full pages, 5 unseen composers, penultimate layer (768 dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.data = data_clas_test_others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, y = learner.get_preds(ds_type=DatasetType.Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_to_composer = {i: composer for i, composer in enumerate(learner.data.classes)}\n",
    "labels = [pred_to_composer[num.item()] for num in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = get_hook_outputs(penult_layer_hook, y, is_penult_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tSNE(points, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. T-SNE plot of full pages, unseen composers, last layer (9 dim, before softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = get_hook_outputs(last_layer_hook, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_tSNE(points, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
